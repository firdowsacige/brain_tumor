{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMRjEWrUCsT5KkJzd01om6k",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/firdowsacige/brain_tumor/blob/main/Untitled17.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PyGtvF_Kpi_u",
        "outputId": "c0d56716-ae43-4505-d5c7-695f01310784"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip install -q git+https://github.com/huggingface/transformers.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from transformers import AutoImageProcessor, DPTForDepthEstimation\n",
        "\n",
        "image_processor = AutoImageProcessor.from_pretrained(\"facebook/dpt-dinov2-small-kitti\")\n",
        "model = DPTForDepthEstimation.from_pretrained(\"facebook/dpt-dinov2-small-kitti\")"
      ],
      "metadata": {
        "id": "jCA-yi00pkaV"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q evaluate"
      ],
      "metadata": {
        "id": "tIAivdqvvn_r"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wget"
      ],
      "metadata": {
        "id": "qO_UEU4rvpPh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the Kvasir-SEG dataset\n",
        "import wget\n",
        "import zipfile"
      ],
      "metadata": {
        "id": "5SUIw5uHv2vl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"http://datasets.simula.no/downloads/kvasir-seg.zip\"\n",
        "zip_path = \"kvasir-seg.zip\"\n",
        "wget.download(url, zip_path)"
      ],
      "metadata": {
        "id": "en9HWpoWv5Ko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the dataset\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(\"/content/kvasir_seg\")"
      ],
      "metadata": {
        "id": "bz2RoW_6v_ql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset, DatasetDict, Image\n",
        "import glob\n",
        "import os\n",
        "\n",
        "# Define your directories (use raw strings for Windows paths)\n",
        "image_dir = r\"/content/kvasir_seg/Kvasir-SEG/images\"\n",
        "mask_dir = r\"/content/kvasir_seg/Kvasir-SEG/masks\"\n",
        "\n",
        "# Collect file paths (adjust the file extension if needed)\n",
        "image_paths = sorted(glob.glob(os.path.join(image_dir, \"*.jpg\")))  # or \"*.png\"\n",
        "mask_paths = sorted(glob.glob(os.path.join(mask_dir, \"*.jpg\")))\n",
        "\n",
        "def create_dataset(image_paths, mask_paths):\n",
        "    # Create a dataset from a dictionary of image and mask paths\n",
        "    dataset = Dataset.from_dict({\"image\": image_paths, \"label\": mask_paths})\n",
        "    # Cast columns to Image objects for proper handling (to load the actual image data)\n",
        "    dataset = dataset.cast_column(\"image\", Image())\n",
        "    dataset = dataset.cast_column(\"label\", Image())\n",
        "    return dataset\n",
        "\n",
        "# Create a dataset for all your data\n",
        "polyp_dataset_all = create_dataset(image_paths, mask_paths)\n",
        "\n",
        "# Now, split the dataset into train and validation sets (e.g., 80%/20%)\n",
        "split_dataset = polyp_dataset_all.train_test_split(test_size=0.2, seed=42)\n",
        "polyp_dataset = DatasetDict({\n",
        "    \"train\": split_dataset[\"train\"],\n",
        "    \"validation\": split_dataset[\"test\"],\n",
        "})\n",
        "\n",
        "# You now have a dataset in the same format as before:\n",
        "# polyp_dataset[\"train\"] and polyp_dataset[\"validation\"] can be used to create your SegmentationDataset wrappers.\n"
      ],
      "metadata": {
        "id": "OIxkJBvLwCJE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_mask(example):\n",
        "    # Convert the segmentation mask to a NumPy array (if it's not already)\n",
        "    mask = np.array(example[\"label\"])\n",
        "\n",
        "    # Convert NumPy array to PyTorch tensor and move to GPU (if available)\n",
        "    mask = torch.tensor(mask).to(device)\n",
        "\n",
        "    # Ensure the mask contains only 0s and 1s\n",
        "    mask = (mask > 0).float()\n",
        "\n",
        "    # Ensure that there are only 0s and 1s in the mask\n",
        "    assert mask.max() <= 1, f\"Mask contains values outside the range [0, 1]: {torch.unique(mask)}\"\n",
        "\n",
        "    # Store the tensor directly in the example (no need to move back to CPU)\n",
        "    example[\"label\"] = mask\n",
        "    return example\n"
      ],
      "metadata": {
        "id": "p1PgmJFXNKfY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "polyp_dataset"
      ],
      "metadata": {
        "id": "aC6smSUxwgZT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example = polyp_dataset[\"train\"][0]\n",
        "image = example[\"image\"]\n",
        "image"
      ],
      "metadata": {
        "id": "EqY7MbKayl2h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "segmentation_map = example[\"label\"]\n",
        "segmentation_map"
      ],
      "metadata": {
        "id": "WqUVfX9YyteF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "id2label = {\n",
        "    0: \"background\",\n",
        "    1: \"polyp\"\n",
        "}\n",
        "print(id2label)"
      ],
      "metadata": {
        "id": "K2grExFizC_D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "import torch\n",
        "\n",
        "class SegmentationDataset(Dataset):\n",
        "  def __init__(self, dataset, transform):\n",
        "    self.dataset = dataset\n",
        "    self.transform = transform\n",
        "  def __len__(self):\n",
        "    return len(self.dataset)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    item = self.dataset[idx]\n",
        "    original_image = np.array(item[\"image\"])\n",
        "    original_segmentation_map = np.array(item[\"label\"])\n",
        "\n",
        "        # Apply transforms\n",
        "    transformed = self.transform(image=original_image, mask=original_segmentation_map)\n",
        "    image = torch.tensor(transformed['image'])\n",
        "    target = torch.LongTensor(transformed['mask'])\n",
        "\n",
        "        # If target has 3 channels, take just one channel\n",
        "    if target.ndim == 3 and target.shape[-1] == 3:\n",
        "      target = target[..., 0]\n",
        "\n",
        "        # Convert image from HWC to CHW\n",
        "    image = image.permute(2, 0, 1)\n",
        "\n",
        "    return image, target, original_image, original_segmentation_map\n"
      ],
      "metadata": {
        "id": "NGynpNIjzaMF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import albumentations as A\n",
        "\n",
        "# Mean and std values for normalization (scaled between 0 and 1)\n",
        "ADE_MEAN = [123.675 / 255, 116.280 / 255, 103.530 / 255]\n",
        "ADE_STD = [58.395 / 255, 57.120 / 255, 57.375 / 255]\n",
        "\n",
        "# Training transformations: Resize, horizontal flip, and normalization.\n",
        "train_transform = A.Compose([\n",
        "    A.Resize(width=448, height=448),\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    A.Normalize(mean=ADE_MEAN, std=ADE_STD),\n",
        "], is_check_shapes=False)\n",
        "\n",
        "\n",
        "val_transform = A.Compose([\n",
        "    A.Resize(width=448, height=448),\n",
        "    A.Normalize(mean=ADE_MEAN, std=ADE_STD),\n",
        "\n",
        "])\n",
        "\n",
        "train_dataset = SegmentationDataset(polyp_dataset[\"train\"], transform=train_transform)\n",
        "val_dataset = SegmentationDataset(polyp_dataset[\"validation\"], transform=val_transform)"
      ],
      "metadata": {
        "id": "5U9q9y4qz2Xs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "pixel_values, target, original_image, original_segmentation_map = train_dataset[3]\n",
        "print(pixel_values.shape)\n",
        "print(target.shape)"
      ],
      "metadata": {
        "id": "IPiXVS0Rz649"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def collate_fn(inputs):\n",
        "    batch = dict()\n",
        "    batch[\"pixel_values\"] = torch.stack([i[0] for i in inputs], dim=0)\n",
        "    batch[\"labels\"] = torch.stack([i[1] for i in inputs], dim=0)\n",
        "    batch[\"original_images\"] = [i[2] for i in inputs]\n",
        "    batch[\"original_segmentation_maps\"] = [i[3] for i in inputs]\n",
        "\n",
        "    return batch\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=2, shuffle=False, collate_fn=collate_fn)"
      ],
      "metadata": {
        "id": "8UI44mfY0kt0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch = next(iter(train_dataloader))\n",
        "for k,v in batch.items():\n",
        "  if isinstance(v,torch.Tensor):\n",
        "    print(k,v.shape)"
      ],
      "metadata": {
        "id": "kzvtyeKB0sRY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(batch[\"labels\"].unique())  # Check the unique values in the labels\n"
      ],
      "metadata": {
        "id": "cqW88EctN9mF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch = next(iter(train_dataloader))\n",
        "for k,v in batch.items():\n",
        "  if isinstance(v,torch.Tensor):\n",
        "    print(k,v.shape)\n",
        "print(batch[\"pixel_values\"].dtype)\n",
        "batch[\"labels\"].dtype"
      ],
      "metadata": {
        "id": "rw1-_QC_0vvD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch[\"pixel_values\"].dtype"
      ],
      "metadata": {
        "id": "meygV5BKGKy9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import Dinov2Model, Dinov2PreTrainedModel\n",
        "from transformers.modeling_outputs import SemanticSegmenterOutput\n",
        "\n",
        "class LinearClassifier(torch.nn.Module):\n",
        "    def __init__(self, in_channels, tokenW=32, tokenH=32, num_labels=1):\n",
        "        super(LinearClassifier, self).__init__()\n",
        "\n",
        "        self.in_channels = in_channels\n",
        "        self.width = tokenW\n",
        "        self.height = tokenH\n",
        "        self.classifier = torch.nn.Conv2d(in_channels, num_labels, (1,1))\n",
        "\n",
        "    def forward(self, embeddings):\n",
        "        embeddings = embeddings.reshape(-1, self.height, self.width, self.in_channels)\n",
        "        embeddings = embeddings.permute(0,3,1,2)\n",
        "\n",
        "        return self.classifier(embeddings)\n",
        "\n",
        "\n",
        "class Dinov2ForSemanticSegmentation(Dinov2PreTrainedModel):\n",
        "  def __init__(self, config):\n",
        "    super().__init__(config)\n",
        "\n",
        "    self.dinov2 = Dinov2Model(config)\n",
        "    self.classifier = LinearClassifier(config.hidden_size, 32, 32, config.num_labels)\n",
        "\n",
        "  def forward(self, pixel_values, output_hidden_states=False, output_attentions=False, labels=None):\n",
        "    # use frozen features\n",
        "    outputs = self.dinov2(pixel_values,\n",
        "                            output_hidden_states=output_hidden_states,\n",
        "                            output_attentions=output_attentions)\n",
        "    # get the patch embeddings - so we exclude the CLS token\n",
        "    patch_embeddings = outputs.last_hidden_state[:,1:,:]\n",
        "\n",
        "    # convert to logits and upsample to the size of the pixel values\n",
        "    logits = self.classifier(patch_embeddings)\n",
        "    logits = torch.nn.functional.interpolate(logits, size=pixel_values.shape[2:], mode=\"bilinear\", align_corners=False)\n",
        "\n",
        "    loss = None\n",
        "    if labels is not None:\n",
        "      # important: we're going to use 0 here as ignore index instead of the default -100\n",
        "      # as we don't want the model to learn to predict background\n",
        "      loss_fct = torch.nn.CrossEntropyLoss(ignore_index=0)\n",
        "      loss = loss_fct(logits.squeeze(), labels.squeeze())\n",
        "\n",
        "    return SemanticSegmenterOutput(\n",
        "        loss=loss,\n",
        "        logits=logits,\n",
        "        hidden_states=outputs.hidden_states,\n",
        "        attentions=outputs.attentions,\n",
        "    )"
      ],
      "metadata": {
        "id": "AnvpNHtT3pSC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Dinov2ForSemanticSegmentation.from_pretrained(\"facebook/dinov2-base\", id2label=id2label, num_labels=len(id2label))"
      ],
      "metadata": {
        "id": "IlPgi5sG3u7V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clamp the labels to ensure they are within [0, 1]\n",
        "batch[\"labels\"] = torch.clamp(batch[\"labels\"], min=0, max=1)"
      ],
      "metadata": {
        "id": "71yC-jGFNmST"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for name, param in model.named_parameters():\n",
        "  if name.startswith(\"dinov2\"):\n",
        "    param.requires_grad = False"
      ],
      "metadata": {
        "id": "7Bh4HGB_3zaL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model(pixel_values=batch[\"pixel_values\"], labels=batch[\"labels\"])\n",
        "print(outputs.logits.shape)\n",
        "print(outputs.loss)"
      ],
      "metadata": {
        "id": "plLUP6T237gu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import AdamW\n",
        "from tqdm.auto import tqdm\n",
        "import torch\n",
        "from torch.nn import BCEWithLogitsLoss\n",
        "import os\n",
        "\n",
        "# Set CUDA_LAUNCH_BLOCKING=1 for debugging purposes\n",
        "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
        "\n",
        "# Training hyperparameters\n",
        "learning_rate = 5e-5\n",
        "epochs = 10\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Put model on GPU (set runtime to GPU in Google Colab)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "\n",
        "# Put model in training mode\n",
        "model.train()\n",
        "\n",
        "# Use BCEWithLogitsLoss for binary classification\n",
        "loss_fn = BCEWithLogitsLoss()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print(\"Epoch:\", epoch)\n",
        "    for idx, batch in enumerate(tqdm(train_dataloader)):\n",
        "        pixel_values = batch[\"pixel_values\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        # Check if labels contain only 0 or 1\n",
        "        assert torch.all((labels == 0) | (labels == 1)), \"Labels contain values other than 0 or 1\"\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(pixel_values)\n",
        "        logits = outputs.logits  # Output logits for binary classification\n",
        "\n",
        "        # Debugging: Check if logits contain NaNs or Infs\n",
        "        assert not torch.any(torch.isnan(logits)), \"Logits contain NaN values\"\n",
        "        assert not torch.any(torch.isinf(logits)), \"Logits contain Inf values\"\n",
        "\n",
        "        # Compute the binary cross-entropy loss\n",
        "        loss = loss_fn(logits.view(-1), labels.view(-1).float())  # Flatten logits and convert labels to float\n",
        "\n",
        "        # Backpropagate the loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Evaluate\n",
        "        with torch.no_grad():\n",
        "            # Apply sigmoid to logits to get probabilities, and convert to binary predictions\n",
        "            predictions = (torch.sigmoid(logits.view(-1)) > 0.5).long()  # 0 or 1 predictions\n",
        "\n",
        "            # Note that the metric expects predictions + labels as numpy arrays\n",
        "            metric.add_batch(predictions=predictions.detach().cpu().numpy(), references=labels.detach().cpu().numpy())\n",
        "\n",
        "        # Print loss and metrics every 100 batches\n",
        "        if idx % 100 == 0:\n",
        "            metrics = metric.compute(num_labels=2,  # Only 2 classes (0 and 1 for binary classification)\n",
        "                                     ignore_index=0,\n",
        "                                     reduce_labels=False)\n",
        "\n",
        "            print(\"Loss:\", loss.item())\n",
        "            print(\"Mean IoU:\", metrics[\"mean_iou\"])\n",
        "            print(\"Mean accuracy:\", metrics[\"mean_accuracy\"])\n"
      ],
      "metadata": {
        "id": "4pRrmliGRKwH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import AdamW\n",
        "from tqdm.auto import tqdm\n",
        "import torch\n",
        "from torch.nn import BCEWithLogitsLoss\n",
        "\n",
        "# Training hyperparameters\n",
        "learning_rate = 5e-5\n",
        "epochs = 10\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Put model on GPU (set runtime to GPU in Google Colab)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "\n",
        "# Put model in training mode\n",
        "model.train()\n",
        "\n",
        "# Use BCEWithLogitsLoss for binary classification\n",
        "loss_fn = BCEWithLogitsLoss()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print(\"Epoch:\", epoch)\n",
        "    for idx, batch in enumerate(tqdm(train_dataloader)):\n",
        "        pixel_values = batch[\"pixel_values\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(pixel_values)\n",
        "        logits = outputs.logits  # Output logits for binary classification\n",
        "\n",
        "        # Compute the binary cross-entropy loss\n",
        "        loss = loss_fn(logits.squeeze(), labels.float())  # Flatten logits and convert labels to float\n",
        "\n",
        "        # Backpropagate the loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Evaluate\n",
        "        with torch.no_grad():\n",
        "            # Apply sigmoid to logits to get probabilities, and convert to binary predictions\n",
        "            predictions = (torch.sigmoid(logits.squeeze()) > 0.5).long()  # 0 or 1 predictions\n",
        "\n",
        "            # Note that the metric expects predictions + labels as numpy arrays\n",
        "            metric.add_batch(predictions=predictions.detach().cpu().numpy(), references=labels.detach().cpu().numpy())\n",
        "\n",
        "        # Print loss and metrics every 100 batches\n",
        "        if idx % 100 == 0:\n",
        "            metrics = metric.compute(num_labels=2,  # Only 2 classes (0 and 1 for binary classification)\n",
        "                                     ignore_index=0,\n",
        "                                     reduce_labels=False)\n",
        "\n",
        "            print(\"Loss:\", loss.item())\n",
        "            print(\"Mean IoU:\", metrics[\"mean_iou\"])\n",
        "            print(\"Mean accuracy:\", metrics[\"mean_accuracy\"])\n"
      ],
      "metadata": {
        "id": "rr9HWhyRQuhj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn import BCEWithLogitsLoss\n",
        "\n",
        "# Define the loss function for binary classification\n",
        "loss_fn = BCEWithLogitsLoss()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print(\"Epoch:\", epoch)\n",
        "    for idx, batch in enumerate(tqdm(train_dataloader)):\n",
        "        pixel_values = batch[\"pixel_values\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        # Debugging: Print unique values of the labels\n",
        "        print(\"Unique labels in batch:\", labels.unique())\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(pixel_values)\n",
        "        logits = outputs.logits  # The raw output logits from the model\n",
        "\n",
        "        # Compute binary cross-entropy loss\n",
        "        loss = loss_fn(logits.view(-1), labels.view(-1).float())  # Flatten for binary classification\n",
        "\n",
        "        # Backpropagate the loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Evaluate the predictions\n",
        "        with torch.no_grad():\n",
        "            # Apply sigmoid to convert logits to probabilities (binary)\n",
        "            predictions = (torch.sigmoid(logits) > 0.5).long()  # Binary prediction (0 or 1)\n",
        "\n",
        "            # Compute metrics\n",
        "            metric.add_batch(predictions=predictions.detach().cpu().numpy(), references=labels.detach().cpu().numpy())\n",
        "\n",
        "        # Print loss and metrics every 100 batches\n",
        "        if idx % 100 == 0:\n",
        "            metrics = metric.compute(num_labels=2,  # Only 2 classes (0 and 1 for binary classification)\n",
        "                                     ignore_index=0,\n",
        "                                     reduce_labels=False)\n",
        "\n",
        "            print(\"Loss:\", loss.item())\n",
        "            print(\"Mean IoU:\", metrics[\"mean_iou\"])\n",
        "            print(\"Mean accuracy:\", metrics[\"mean_accuracy\"])\n"
      ],
      "metadata": {
        "id": "bONdo3HCQQJ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import AdamW\n",
        "from tqdm.auto import tqdm\n",
        "import torch\n",
        "\n",
        "# Training hyperparameters\n",
        "learning_rate = 5e-5\n",
        "epochs = 10\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Put model on GPU (set runtime to GPU in Google Colab)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "\n",
        "# Put model in training mode\n",
        "model.train()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print(\"Epoch:\", epoch)\n",
        "    for idx, batch in enumerate(tqdm(train_dataloader)):\n",
        "        pixel_values = batch[\"pixel_values\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        # Debugging: Print unique values of the labels\n",
        "        print(\"Unique labels in batch:\", labels.unique())\n",
        "\n",
        "        # Ensure labels are in the correct range [0, 1] for binary segmentation\n",
        "        assert labels.max() <= 1, \"Labels contain values outside of the expected range for binary segmentation.\"\n",
        "        batch[\"labels\"] = torch.clamp(batch[\"labels\"], min=0, max=1)  # Fix labels if necessary\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(pixel_values, labels=labels)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        # Backpropagate the loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Evaluate the predictions\n",
        "        with torch.no_grad():\n",
        "            logits = outputs.logits\n",
        "            # Apply sigmoid activation to convert logits to probabilities (binary)\n",
        "            predictions = (torch.sigmoid(logits) > 0.5).long()  # Binary prediction (0 or 1)\n",
        "\n",
        "            # Compute metrics\n",
        "            metric.add_batch(predictions=predictions.detach().cpu().numpy(), references=labels.detach().cpu().numpy())\n",
        "\n",
        "        # Print loss and metrics every 100 batches\n",
        "        if idx % 100 == 0:\n",
        "            metrics = metric.compute(num_labels=2,  # Only 2 classes (0 and 1 for binary classification)\n",
        "                                     ignore_index=0,\n",
        "                                     reduce_labels=False)\n",
        "\n",
        "            print(\"Loss:\", loss.item())\n",
        "            print(\"Mean IoU:\", metrics[\"mean_iou\"])\n",
        "            print(\"Mean accuracy:\", metrics[\"mean_accuracy\"])\n"
      ],
      "metadata": {
        "id": "KTqMpnKCN-oD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import AdamW\n",
        "from tqdm.auto import tqdm\n",
        "import torch\n",
        "from torch.nn import BCEWithLogitsLoss\n",
        "\n",
        "# Training hyperparameters\n",
        "learning_rate = 5e-5\n",
        "epochs = 10\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Put model on GPU (set runtime to GPU in Google Colab)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "\n",
        "# Put model in training mode\n",
        "model.train()\n",
        "\n",
        "# Use BCEWithLogitsLoss for binary classification\n",
        "loss_fn = BCEWithLogitsLoss()\n",
        "\n",
        "\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print(\"Epoch:\", epoch)\n",
        "    for idx, batch in enumerate(tqdm(train_dataloader)):\n",
        "        pixel_values = batch[\"pixel_values\"].to(device, dtype=torch.float32)  # Typecast to float32\n",
        "        labels = batch[\"labels\"].to(device, dtype=torch.float32)  # Typecast to float32\n",
        "\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(pixel_values)\n",
        "        logits = outputs.logits  # Output logits for binary classification\n",
        "\n",
        "        # Compute the binary cross-entropy loss\n",
        "        loss = loss_fn(logits.view(-1), labels.view(-1).float())  # Flatten logits and convert labels to float\n",
        "\n",
        "        # Backpropagate the loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Evaluate\n",
        "        with torch.no_grad():\n",
        "            # Apply sigmoid to logits to get probabilities, and convert to binary predictions\n",
        "            predictions = (torch.sigmoid(logits.view(-1)) > 0.5).long()  # 0 or 1 predictions\n",
        "\n",
        "            # Note that the metric expects predictions + labels as numpy arrays\n",
        "            metric.add_batch(predictions=predictions.detach().cpu().numpy(), references=labels.detach().cpu().numpy())\n",
        "\n",
        "        # Print loss and metrics every 100 batches\n",
        "        if idx % 100 == 0:\n",
        "            metrics = metric.compute(num_labels=2,  # Only 2 classes (0 and 1 for binary classification)\n",
        "                                     ignore_index=0,\n",
        "                                     reduce_labels=False)\n",
        "\n",
        "            print(\"Loss:\", loss.item())\n",
        "            print(\"Mean IoU:\", metrics[\"mean_iou\"])\n",
        "            print(\"Mean accuracy:\", metrics[\"mean_accuracy\"])\n"
      ],
      "metadata": {
        "id": "O-ucOLzwR8EN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}